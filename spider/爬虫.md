# 爬虫
##写这篇文章的目的
简单的介绍爬虫的基本概念。这篇文章主要是围绕了自己需求爬虫而写。而不是建立那种搜索引擎的爬虫。

这篇文章假设只是为了实现抓取网易新闻（代码还没写）。
##爬虫是什么？
网络爬虫又叫做网络蜘蛛，网络机器人。是一个程序，会自动的抓取网络上的网页。有人用来把网页上的相关数据保存下来，做成搜索引擎。我的目的是用它来抓取我所感兴趣的资源，如查工作网站上的职位信息，抓取某个网站上的资源。
	
[维基百科](https://en.wikipedia.org/wiki/Web_crawler) 

##爬虫能干什么？
爬虫的不仅仅是大公司能用来做搜索引擎。爬虫的目的是采集数据，采集到的数据可以怎么使用，就可以天马行空了。

例如：

- 爬取各个电影论坛，获取最新电影电下载地址，建立搜索服务，用户通过公众号搜索电影名称，可以检索到已经采集到的电影电下载地址，使用迅雷下载，观看电影。
- 抓取各个新闻网站的新闻，用于作为自己的资源，作为自己应用的内容或者公众号资源。
　	

##爬虫的分类
爬虫按照系统结构和实现技术大致可以分为：
> 通用网络爬虫（General Purpose Web Crawler）、聚焦网络爬虫（Focused Web Crawler）、增量式网络爬虫（Incremental Web Crawler）、深层网络爬虫（Deep Web Crawler）。 实际的网络爬虫系统通常是几种爬虫技术相结合实现的

这里不讨论，因为只是为了做满足一些自己需求的小爬虫。
##　一般的爬虫框架
![通用框架](https://github.com/75509151/python_storage/blob/master/spider/data/20150109220002921.png) 

### 爬虫的基本流程
1. 选取一部分种子url.
2. 将这些url放入待抓取的url队列。
3. 从待抓取队列取出url,将对应的网页下载下来，并存储到网页库中。然后将其放入已经抓取的url队.
4. 分析已经抓取队列中的url，分析其中的url,将未抓取的放入待抓取队列，然后进入下一个循环。

[博客园](http://www.cnblogs.com/wawlian/archive/2012/06/18/2553061.html)

 这里讨论爬虫的基本流程只是为了明白通用爬虫的基本框架，借鉴它来实现我们需要的爬虫。

##爬虫程序设计思路
[360](http://www.360doc.com/content/11/0109/14/3740482_85208865.shtml)
[csdn](http://blog.csdn.net/historyasamirror/article/details/7061059)
看了爬虫的基本流程和基本设计思路，总结起来大致流程就是：根据url将网页下载下来，然后提取里面的url,再根据这些新url对应的下载对应的网页，不断循环。
　可是这个有个新的问题，对于我们的需求，需要下载某一类资源或者保存网页内的url该如何实现呢。
　其实为了完成我们的需求，我们需要将我们的爬虫项目分成几个模块。
　- Fetcher: 根据url下载网页。
　- DNS Resolver: DNS解析。
　- Content Seen 网页内容去重
　- Extractor: 提取网页中的url或者其他内容
　- URL Filter 过滤不需要下载的url
　- URL Seen : url 去重
　- URL Set : 存储所有的URL
　- URL Frontier： 调度器，决定接下来哪些下载哪些url对应的网页
　- 
　```
　```
##扩展
xpath
selenium

##讨论
[知乎](https://www.zhihu.com/question/20899988)

## 摘抄
> Fetcher和DNS Resolver
这两个模块是两个非常简单的独立的服务：DNS Resolver负责域名的解析；Fetcher的输入是域名解析后的url，返回的则是该url对应的网页内容。对于任何一次网页的抓取，它都需要调用这两个模块。
对一般的爬虫，两个模块可以做得非常的简单，甚至合并到一起。但是对于性能要求很高的系统，它们可能成为潜在的性能瓶颈。主要原因是无论是域名解析还是抓取，都是很耗时的工作。比如抓取网页，一般的延迟都在百毫秒级别，如果遇上慢的网站，可能要几秒甚至十几秒，这导致工作线程会长时间的处于阻塞等待的状态。如果希望Fetcher能够达到每秒几千个网页甚至更高的下载，就需要启动大量的工作线程。
因此，对于性能要求高的爬虫系统，一般会采用epoll或者类似的技术将两个模块改成异步机制。另外，对于DNS的解析结果也会缓存下来，大大降低了DNS解析的操作。
>Content Seen
Internet上的一些站点常常存在着镜像网站（mirror），即两个网站的内容一样但网页对应的域名不同。这样会导致对同一份网页爬虫重复抓取多次。为了避免这种情况，对于每一份抓取到的网页，它首先需要进入Content Seen模块。该模块会判断网页的内容是否和已下载过的某个网页的内容一致，如果一致，则该网页不会再被送去进行下一步的处理。这样的做法能够显著的降低爬虫需要下载的网页数。
至于如果判断两个网页的内容是否一致，一般的思路是这样的：并不会去直接比较两个网页的内容，而是将网页的内容经过计算生成FingerPrint（指纹），通常FingerPrint是一个固定长度的字符串，要比网页的正文短很多。如果两个网页的FingerPrint一样，则认为它们内容完全相同。

>Extractor和Url Filter
Extractor的工作是从下载的网页中将它包含的所有url提取出来。这是个细致的工作，你需要考虑到所有可能的url的样式，比如网页中常常会包含相对路径的url，提取的时候需要将它转换成绝对路径。
Url Filter则是对提取出来的url再进行一次筛选。不同的应用筛选的标准是不一样的，比如对于baidu/google的搜索，一般不进行筛选，但是对于垂直搜索或者定向抓取的应用，那么它可能只需要满足某个条件的url，比如不需要图片的url，比如只需要某个特定网站的url等等。Url Filter是一个和应用密切相关的模块。

>Url Seen
Url Seen用来做url去重。关于url去重之前已经写过一篇blog，这里就不再详谈了。
对于一个大的爬虫系统，它可能已经有百亿或者千亿的url，新来一个url如何能快速的判断url是否已经出现过非常关键。因为大的爬虫系统可能一秒钟就会下载几千个网页，一个网页一般能够抽取出几十个url，而每个url都需要执行去重操作，可想每秒需要执行大量的去重操作。因此Url Seen是整个爬虫系统中非常有技术含量的一个部分。（Content Seen其实也存在这个问题）

>Url Set
当url经过前面的一系列处理后就会被放入到Url Set中等待被调度抓取。因为url的数量很大，所以只有一小部分可能被放在内存中，而大部分则会写入到硬盘。一般Url Set的实现就是一些文件或者是数据库。

>URL Frontier
Frontier（为什么叫这个名字我也不清楚）之所以放在最后，是因为它可以说是整个爬虫系统的引擎和驱动，组织和调用其它的模块。
当爬虫启动的时候，Froniter内部会有一些种子url，它先将种子url送入Fetcher进行抓取，然后将抓取下来的网页送入Extractor提取新的url，再将新的url去重后放入到Url Set中；而当Froniter内部的url都已经抓取完毕后，它又从Url Set中提取那些新的没有被抓取过的url，周而复始。
>Frontier的调度实现有很多种，这里只介绍最常见的一种实现方法。在此之前，需要先解释一点，尽管在介绍Fetcher的时候我们说，好的Fetcher每秒能够下载百千个网页，但是对于某个特定的目标网站，比如www.sina.com，爬虫系统对它的抓取是非常慢速的，十几秒才会抓取一次，这是为了保证目标网站不至于被爬虫给抓垮。
为了做到这一点，Frontier内部对于每个域名有一个对应的FIFO队列，这个队列保存了该域名下的url。Frontier每次都会从某个队列中拿出一个url进行抓取。队列会保存上一次被Frontier调用的时间，如果该时间距离现在已经超过了一定值，那么该队列才可以再次被调用。
Frontier内部同时可能拥有成千上万个这样的队列，它会轮询的获取一个可以被调用的队列，然后从该队列中pull一个url进行抓取。而一旦所有队列中的url被消耗到一定程度，Frontier又会从Url Set中提取一批新的url放入对应的队列。

>分布式
当单机版的爬虫性能不能满足要求的时候，就应该考虑用多台机器组成分布式的爬虫系统。分布式的爬虫架构其实要比想象的简单得多，一个朴素的做法是：假设有N台机器，每台机器上有运行了一个完整的爬虫系统，每台机器的爬虫在从Extractor模块获得新的url之后，根据url的域名进行hash然后取模N得到结果n，然后该url会被放入第n台机器的Url Set中。这样，不同网站的url会被放在不同的机器上处理。

>以上就是一个完整爬虫的系统实现。当然，由于篇幅有限回避了一些细节。比如爬虫抓取每个网站前需要先读取该网站的robots.txt来判断该网站是否允许被抓取（前段时间京东就在robots.txt中将一淘的爬虫屏蔽了，需要说明的是，robots.txt只是一个业内约定，它并不能从技术上强制的拒绝爬虫的抓取）；再比如，一些网站提供了sitemap，这样可以直接从sitemap上获取该网站的所有url；等等。